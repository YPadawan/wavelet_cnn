{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9996b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd11a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import helper\n",
    "import pywt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c2fdf",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of the notebook is to implement the architecture shown in the following article.\n",
    "\n",
    "*Wavelet Convolutional Neural Networks*, Fujieda et al. 2018, (link [here](https://arxiv.org/abs/1805.08620))\n",
    "\n",
    "Basically the wavelet CNN is a convolutional neural network with two main characteristics:\n",
    "- The first one is that the data entering the network are not images, but **their wavelet decomposition**. A first step is to always decompose the input images by passing them into a low pass filter and high pass filter.\n",
    "    - Everytime, before being passed to the convolution blocks, both low-pass and high-pass filters are concatenated channel-wise.\n",
    "    - The low-pass filter is recursively decomposed before each level.\n",
    "    - After the first level, the data is again concatenated but with another component the shortcut projection.\n",
    "- The second one, is the use of shortcuts projections.\n",
    "    - These shorcuts are added to the channel-wise concatenation before each level of convolution\n",
    "    - The more forward we go in the network, the more convolutions the filter have to pass through.\n",
    "   \n",
    "\n",
    "From the architecture described above, it is possible to decompose the network into basic building blocks. Following an Object-Oriented Programming, it will be possible to build classes for each basic block.\n",
    "The building blocks would be as follows:\n",
    "\n",
    "- **The decomposition block**, data decomposition into low-pass filters and high-pass filters\n",
    "- **The shortcut block**, shortcuts used in the process\n",
    "- **The concatenation block**, channel-wise concatenation that occurs before the convolutions\n",
    "- **The convolution block**, a two-step convolution that occurs at each level\n",
    "\n",
    "The purpose of the notebook is to work on each of these blocks and put them together to create the wavelet-CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e075f",
   "metadata": {},
   "source": [
    "# First and foremost, reading the data\n",
    "\n",
    "Here is the link to download the data: https://www.csc.kth.se/cvap/databases/kth-tips/download.html\n",
    "\n",
    "Even though it is not very important...\n",
    "\n",
    "The data used in the example is the same that has been used in the article. k-th-tips-2. The authors just read the data in the regular way.\n",
    "- They use regular images of size 224x224\n",
    "- A first step is to scale the training images to 256x256\n",
    "- then conducting random crops to 224x224\n",
    "- Then flipping\n",
    "\n",
    "It is done like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../wavelets/kth-tips2-b_col_200x200/\"\n",
    "transform = transforms.Compose([transforms.Resize(255), \n",
    "                               transforms.CenterCrop(224),\n",
    "                               transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder(data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, \n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13007e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155c4f2",
   "metadata": {},
   "source": [
    "The function `imshow` below has been taken from a stackoverflow discussion because it allowed to show the images (link [here](https://stackoverflow.com/questions/53570181/error-in-importing-libraries-of-helper-though-helper-is-installed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(images[0].view(3, 224, 224));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66f4b9",
   "metadata": {},
   "source": [
    "## Decomposition block\n",
    "\n",
    "### Convolution and convolutional networks in a nutshell\n",
    "\n",
    "A brief explanation of convolutions is relevant in order to justify why the use of wavelets can be relevant in image analysis. The authors justify the use of wavelets based on the limits of the regular convolution operation in a CNN. In the CNN litterature a convolution operation is just like this (GoodFellow et al, 2016, link [here](https://www.deeplearningbook.org/contents/convnets.html)):\n",
    "\n",
    "$$\n",
    "s(t) = (x * w)(t) = \\sum_{a=-\\infty}^{+\\infty}x(a)w(t -a)\n",
    "$$\n",
    "\n",
    "The first argument $x$ is called the **input** and the second one $w$ is the **kernel**. The output $s(t)$ is called the **feature map**. In general in a CNN each convolution layer contains several operations added to the one described above. It is very often associated with for example a **pooling operation**, a normalization, and an activation function.\n",
    "\n",
    "### A word on multiresolution analysis\n",
    "\n",
    "Before talking about the technical aspect of it, let us briefly explain how multiresolution signal works. The multiresolution analysis (MRA) has been introduced by St√©phane MALLAT in 1989 in the paper *A Theory of Multiresolution Signal Decomposition* (link [here](https://www.di.ens.fr/~mallat/papiers/MallatTheory89.pdf)). The Multiresolution approah consists in decomposing images into approximation and details coefficients at resolution based on $2^j$ (the standard one, and the one used in Fujieda et al). In Mallat's paper, the signal of an image can be decomposed using a *scaling function* $\\phi(x)$ and a *wavelet function* $\\psi(x)$. The scaling function is used to compute the approximation signal whereas the wavelet function computes the detail signal. It is worth mentionning that the approximation can be considered like a low-pass filter of the image signal, whereas the detail signal corresponds to a high-pass filter.\n",
    "\n",
    "\n",
    "### Wavelets in a CNN\n",
    "Fujieda et al, go from the basic convolution layer that occurs in a CNN. But they note that CNNs can be seen as a limited form of multiresolution analysis because it does not take into account the high-pass filtered information. The convolution for them is noted as such:\n",
    "$$\n",
    "x_{l, t+1} = (x_{l, t} * k) \\downarrow 2\n",
    "$$\n",
    "\n",
    "Where $x_{l, t}$ denotes the value of $x$ at the layer $l$ at time $t$, and $\\downarrow 2$ implies a downsizing of the output to half the original size.\n",
    "\n",
    "In the equation above the authors considered that a CNN would a limited form of multiresolution analysis. Using a a wavelet decomposition of an image would allow to take into account the whole spectrum of the multiresolution analysis and the hierarchical decomposition of an image. So now the convolution becomes:\n",
    "\n",
    "$$\n",
    "x_l = (x_{l, t} * k_{l, t}) \\downarrow 2\n",
    "$$\n",
    "$$\n",
    "x_h = (x_{l, t} * k_{h, t}) \\downarrow 2\n",
    "$$\n",
    "\n",
    "In Mallat's approach, $k_{l, t}$ corresponds to the scaling function $\\phi(x)$ (the approximation coefficients) and $k_{h, t}$ is the wavelet function $\\psi(x)$ (details coefficients)\n",
    "\n",
    "### Implementation with pywt\n",
    "This is considered to be the most important block, since it is the main innovation brought by the architecture. In parallel to the convolution filters, a mutli-level decomposition of the image takes place. In figure 1 of the paper, a 4-level decomposition of the image takes place. Actually as mentionned earlier even convolution layers are trained on the wavelet transform of the image.\n",
    "- At each step, only the low-pass filters are further decomposed, in multi-resolution analysis, it corresponds to the approximation coefficients that are to be recursively decomposed at each level (with a decrease of the size by half at each level $2^{-j}$).\n",
    "\n",
    "- It is possible to use the `pywt` package to perform this decomposition. There is a method that makes this decomposition and maybe even returns the concatenated version of the filters. Useful methods would be is `pywt.coeffs_to_array` and `wavedec2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7deedd",
   "metadata": {},
   "source": [
    "## The shortcut block\n",
    "\n",
    "The shortcut block is taken directly from the paper about *Deep Residual Learning*, He et al, 2016 (link [here](https://arxiv.org/pdf/1512.03385.pdf)). Basically it involves an identity mapping of the inputs that are then added to the output layer. Let us admit a mapping function $\\mathcal{F}(x)$ that corresponds to the feature map of a convolution layer for example. Before entering the next layer, the whole input $x$ is added to the mapping $\\mathcal{F}(x)$ which gives: \n",
    "$$\\mathcal{F}(x) + x$$\n",
    "\n",
    "This is what He et al, have called the *residual mapping* and note (interestingly) that the identity shorcut connections add neither extra parameters nor computational complexity. Except that in our case, identity shortcuts are not adapted because dimensions differe between input and output layers. So this is where **projection shortcuts** come into play. Let us write the way *He et al*, present the projection shortcut:\n",
    "$$\n",
    "\\mathbf{y} = \\mathcal{F}(x, \\{W_i\\}) + W_s \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{F}(x, \\{W_i\\})$ is the residual mapping to be learned and $W_s$ is a linear projection for $\\mathbf{x}$ used in order to match the dimensions of the residual mapping. Fujieda et al, indicate that the projection shortcuts are performed using a 1x1 convolutional kernel in order to increase dimensions of the input. All shortcuts are done using projection shortcuts. But it differs in between the first shortcut and the rest.\n",
    "\n",
    "- The first shortcut projection is made with the highpass filter of the first level decomposition. This filter is concatenated to other data especially before entering the second layer. *(still a doubt that)*\n",
    "- For all the other levels the whole decomposition is projected and concatenated.\n",
    "\n",
    "Below a `ProjectionShortcut` class has been developped in order to use it later for the wavelet CNN. It might be a bit *overkill* to create a class just for a projection shortcut but due to a lack of PyTorch mastery, it seems to be the best solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757464bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionShortcut(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                              kernel_size=1, stride=self.stride, padding=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a216a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a dimension to x before convolution because Conv2d accepts batches of data\n",
    "x= x.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b1a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = ProjectionShortcut(3, 64)\n",
    "ps(x)\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ffe97",
   "metadata": {},
   "source": [
    "## Concatenation block\n",
    "\n",
    "Despite the use of shortcut connections, it appears that a vanishing gradient problem can still appear in the process. The authors have relied on a method taken from *DenseNet* an architecture which has been presented in *Densely Connected Convolutional Networks*, Huang et al, 2018 (link [here](https://arxiv.org/pdf/1608.06993.pdf)). The operation here is very basic, it relies on a **channel-wise concatenation** of several tensors. In the very particular case of the Wavelet-CNN, it is the concatenation of the output of the previous layer with  a feature map of the wavelet decomposition of the image at corresponding level.\n",
    "\n",
    "### Concatenating pytorch tensors\n",
    "\n",
    "There are functions that can do the concatenation in Pytorch (e.g. `torch.cat` or `torch.stack`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547863e1",
   "metadata": {},
   "source": [
    "## Convolution block\n",
    "- The decomposition block which corresponds to the convolution filter that are used. The example is a four level decomposition of the input image.\n",
    "- But it is always the same block with different *in_channels* and *out_channels* size\n",
    "- So as a first step, a basic `ConvolutionBlock` class is built before using it in the development of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Convolution layer\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.in_channels, \n",
    "                      out_channels=self.out_channels, \n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.out_channels, \n",
    "                      out_channels=self.out_channels, \n",
    "                      kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a43db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.ones((1, 3, 224, 224))\n",
    "block = ConvolutionBlock(3, 64)\n",
    "block(dummy)\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc6f96",
   "metadata": {},
   "source": [
    "# Bringing things together\n",
    "\n",
    "Below is *(an attempt)* at creating a version for the Wavelet-CNN, for this a class `WaveletCnn` is created in which the different components of the network will be taken into consideration.\n",
    "\n",
    "As it is just a basic example aiming to reproduce the network as it has been shown in Fujieda et al. Their model in Fig. 1 in the paper consists of a CNN with a 4-level decomposition of the input image, hence four corresponding convolution layers with a global average pooling layer and a fully connected layer at the end.\n",
    "\n",
    "- There is a method `img_decomposition` within the class that should allow to make the image decomposition and creates an iterable containing the different levels of the multiresolution analysis.\n",
    "\n",
    "**REMINDER**: The code below is unfinished (just like the whole project) its only purpose is to show how the architecture the authors suggested would look like. There are still many problems to fix including (but not only):\n",
    "- The handling of the image decomposition within the class\n",
    "- How the projection shortcuts work and where do we perform the addition (if there is one)\n",
    "- How to handle global average pooling\n",
    "- Perhaps a special class should be created to the handle the wavelet decomposition which would allow us to extract the relvant arrays at the corresponding level\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletCnn(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Higher part of the projection shortcuts, with stride 2\n",
    "        self.ps1 = ProjectionShortcut(3, 64, 2)\n",
    "        self.ps2 = ProjectionShortcut(64, 128, 2)\n",
    "        self.ps3 = ProjectionShortcut(128, 256, 2)\n",
    "        self.ps4 = ProjectionShortcut(256, 512, 2)\n",
    "        \n",
    "        # Creating four convolution blocks\n",
    "        \n",
    "        self.block1 = ConvolutionBlock(3, 64)\n",
    "        self.block2 = ConvolutionBlock(64, 128)\n",
    "        self.block3 = ConvolutionBlock(128, 256)\n",
    "        self.block4 = ConvolutionBlock(256, 512)\n",
    "        \n",
    "        # Transformations for decomposition\n",
    "        self.convdec2 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        self.convdec3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, \n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, \n",
    "                      out_channels=128, \n",
    "                      kernel_size=3, padding=1))\n",
    "        \n",
    "        self.convdec4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, \n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, \n",
    "                      out_channels=128, \n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, \n",
    "                      out_channels=256, \n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        # Projection shortcuts for the wavelet decomposition\n",
    "        self.psdec2 = ProjectionShortcut(3, 64)\n",
    "        self.psdec3 = ProjectionShortcut(3, 128)\n",
    "        self.psdec4 = ProjectionShortcut(3, 256)\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc2 = nn.Linear(512, 11)\n",
    "\n",
    "        \n",
    "          \n",
    "    def img_decomposition(self, img, n_level):\n",
    "        \"\"\"Function performing the multiresolution decomposition at n_level \n",
    "        and returning a dictionary with the corresponding arrays and slices\"\"\"\n",
    "        \n",
    "        def decomposition(img, level):\n",
    "            c = pywt.wavedec2(x, 'Haar', mode='periodization', level=level)\n",
    "            arr, slices = pywt.coeffs_to_array(c, axes=[1, 2])\n",
    "            return arr, slices\n",
    "        \n",
    "        decomposition_dict = {\"level\"+str(l+1): decomposition(img, l) for l in range(n_level)}\n",
    "        \n",
    "        return decomposition_dict\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        dec_dict = self.img_decomposition(x, 4)\n",
    "        # fetching the detail coefficient of the first level decomposition\n",
    "        d1 = dec_dict[\"level1\"][0][dec_dict[\"level1\"][1][1][\"dd\"]]\n",
    "        ps1 = self.ps1(d1)\n",
    "        arr = dec_dict[\"level1\"][0]\n",
    "        \n",
    "        # First level k1\n",
    "        x = self.block1(arr1)\n",
    "        psdec2 = self.psdec2(dec_dict[\"level2\"][0])\n",
    "        convdec2 = self.convdec2(dec_dict[\"level2\"][0])\n",
    "        x = torch.cat([x, ps1, psdec2, convdec1], 1)\n",
    "        ps2 = self.ps2(x)\n",
    "        \n",
    "        # Second layer k2\n",
    "        x = self.block2(x)\n",
    "        psdec3 = self.psdec3(dec_dict[\"level3\"][0])\n",
    "        convdec3 = self.convdec3(dec_dict[\"level3\"][0])\n",
    "        x = torch.cat([x, ps2, psdec3, convdec3], 1)\n",
    "        ps3 = self.ps3(x)\n",
    "        \n",
    "        # Third layer k3\n",
    "        x = self.block3(x)\n",
    "        psdec4 = self.psdec4(dec_dict[\"level4\"][0])\n",
    "        convdec4 = self.convdec4(dec_dict[\"level4\"][0])\n",
    "        x = torch.cat([x, ps3, psdec4, convdec4], 1)\n",
    "        ps4 = self.ps4(x)\n",
    "        \n",
    "        # Fourth layer k4 (final one before average pooling)\n",
    "        x = self.block4(x)\n",
    "        x = torch.cat([x, ps4], 1)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # returning the prediction\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
